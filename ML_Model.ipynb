{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bd414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, mean_squared_error, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c38bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\punee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\punee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\punee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: heart, pink, light, hanging, holder, decoration, rose, box, cream, green\n",
      "Topic 1: metal, sign, card, cake, blue, pack, wooden, case, pink, candle\n",
      "Topic 2: bag, retrospot, red, design, lunch, bottle, jumbo, hot, water, home\n",
      "Topic 3: set, tin, christmas, cake, design, light, pantry, tea, wall, tree\n",
      "Topic 4: vintage, set, bag, paper, christmas, girl, doily, ribbon, dolly, feltcraft\n",
      "Topics assigned and saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Data/Processed_data/regular_transactions.csv\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text) \n",
    "    tokens = text.split()  \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]  \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens] \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['ProductName_clean'] = df['ProductName'].apply(preprocess_text)\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(df['ProductName_clean'])\n",
    "\n",
    "# Fit LDA Model\n",
    "n_topics = 5\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda_model.fit(dtm)\n",
    "\n",
    "# Assign topics to products\n",
    "topic_assignments = lda_model.transform(dtm)  # Get topic distribution for each product\n",
    "df['Topic'] = topic_assignments.argmax(axis=1)  # Assign most probable topic\n",
    "\n",
    "# Function to display topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topics[topic_idx] = top_features\n",
    "    return topics\n",
    "\n",
    "# Get top words for each topic\n",
    "no_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "topics = display_topics(lda_model, feature_names, no_top_words)\n",
    "\n",
    "# Print topics\n",
    "for topic_num, top_words in topics.items():\n",
    "    print(f'Topic {topic_num}: {\", \".join(top_words)}')\n",
    "\n",
    "# Save output CSV\n",
    "df.to_csv(\"Data/Processed_data/regular_transactions_with_topics.csv\", index=False)\n",
    "\n",
    "print(\"Topics assigned and saved to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66dedba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\punee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\punee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\punee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: heart, pink, light, hanging, holder, decoration, rose, box, cream, green\n",
      "Topic 1: metal, sign, card, cake, blue, pack, wooden, case, pink, candle\n",
      "Topic 2: bag, retrospot, red, design, lunch, bottle, jumbo, hot, water, home\n",
      "Topic 3: set, tin, christmas, cake, design, light, pantry, tea, wall, tree\n",
      "Topic 4: vintage, set, bag, paper, christmas, girl, doily, ribbon, dolly, feltcraft\n",
      "Topics assigned and saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Data/Processed_data/regular_transactions.csv\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text) \n",
    "    tokens = text.split()  \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]  \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens] \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['ProductName_clean'] = df['ProductName'].apply(preprocess_text)\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(df['ProductName_clean'])\n",
    "\n",
    "# Fit LDA Model\n",
    "n_topics = 5\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda_model.fit(dtm)\n",
    "\n",
    "# Assign topics to products\n",
    "topic_assignments = lda_model.transform(dtm)  # Get topic distribution for each product\n",
    "df['Topic'] = topic_assignments.argmax(axis=1)  # Assign most probable topic\n",
    "\n",
    "# Function to display topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topics[topic_idx] = top_features\n",
    "    return topics\n",
    "\n",
    "# Get top words for each topic\n",
    "no_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "topics = display_topics(lda_model, feature_names, no_top_words)\n",
    "\n",
    "# Print topics\n",
    "for topic_num, top_words in topics.items():\n",
    "    print(f'Topic {topic_num}: {\", \".join(top_words)}')\n",
    "\n",
    "# Save output CSV\n",
    "df.to_csv(\"Data/Processed_data/regular_transactions_with_topics.csv\", index=False)\n",
    "\n",
    "print(\"Topics assigned and saved to CSV.\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d8f20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and prepare the data\n",
    "def load_data():\n",
    "    df = pd.read_csv('Data/Processed_data/regular_transactions_with_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d591e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Customer Segmentation Using K-means Clustering\n",
    "def customer_segmentation(df, n_clusters=4):\n",
    "    \"\"\"Segment customers based on their purchasing behavior\"\"\"\n",
    "\n",
    "    customer_features = df.groupby('CustomerNo').agg({\n",
    "        'TransactionNo': 'nunique',  \n",
    "        'Revenue': 'sum',            \n",
    "        'Quantity': 'sum',           \n",
    "        'ProductNo': 'nunique',     \n",
    "        'Price': ['mean', 'max']     \n",
    "    })\n",
    "    \n",
    "    # Flatten multi-level column names\n",
    "    customer_features.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in customer_features.columns]\n",
    "    \n",
    "    # Additional derived metrics\n",
    "    customer_features['AvgBasketSize'] = customer_features['Quantity_sum'] / customer_features['TransactionNo_nunique']\n",
    "    customer_features['AvgTransactionValue'] = customer_features['Revenue_sum'] / customer_features['TransactionNo_nunique']\n",
    "    \n",
    "    # For clustering, use only numeric features\n",
    "    cluster_features = [\n",
    "        'TransactionNo_nunique', 'Revenue_sum', 'Quantity_sum', \n",
    "        'ProductNo_nunique', 'Price_mean', 'Price_max',\n",
    "        'AvgBasketSize', 'AvgTransactionValue'\n",
    "    ]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(customer_features[cluster_features])\n",
    "    \n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=min(len(cluster_features), 3))\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    customer_features['Cluster'] = kmeans.fit_predict(X_pca)\n",
    "    \n",
    "    # Analyze clusters\n",
    "    cluster_analysis = customer_features.groupby('Cluster').agg({\n",
    "        'TransactionNo_nunique': 'mean',\n",
    "        'Revenue_sum': 'mean',\n",
    "        'Quantity_sum': 'mean',\n",
    "        'ProductNo_nunique': 'mean',\n",
    "        'Price_mean': 'mean',\n",
    "        'AvgBasketSize': 'mean',\n",
    "        'AvgTransactionValue': 'mean'\n",
    "    }).sort_values('Revenue_sum', ascending=False)\n",
    "    \n",
    "    # Create descriptive cluster names\n",
    "    if len(cluster_analysis) >= 4:\n",
    "        cluster_names = {\n",
    "            cluster_analysis.index[0]: \"High-Value Customers\",\n",
    "            cluster_analysis.index[1]: \"Regular Customers\",\n",
    "            cluster_analysis.index[2]: \"Occasional Shoppers\",\n",
    "            cluster_analysis.index[3]: \"Low-Value Customers\"\n",
    "        }\n",
    "    else:\n",
    "        # If fewer clusters, assign basic names\n",
    "        cluster_names = {i: f\"Segment {i+1}\" for i in range(len(cluster_analysis))}\n",
    "    \n",
    "    # Add cluster names to the customer features\n",
    "    customer_features['SegmentName'] = customer_features['Cluster'].map(cluster_names)\n",
    "    \n",
    "    return customer_features, cluster_analysis, cluster_names, pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21fbdebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Product Association Analysis (Market Basket Analysis)\n",
    "def product_association_analysis(df):\n",
    "    \"\"\"Analyze which products are frequently purchased together\"\"\"\n",
    "    # Create transaction-product matrix\n",
    "    transaction_product_matrix = pd.crosstab(df['TransactionNo'], df['ProductNo'])\n",
    "    \n",
    "    # Convert to binary (purchased or not)\n",
    "    transaction_product_binary = (transaction_product_matrix > 0).astype(int)\n",
    "    \n",
    "    # Calculate product support (percentage of transactions containing the product)\n",
    "    product_support = transaction_product_binary.sum() / len(transaction_product_binary)\n",
    "    \n",
    "    # Calculate confidence and lift for product pairs\n",
    "    associations = []\n",
    "    \n",
    "    for prod1 in transaction_product_binary.columns:\n",
    "        for prod2 in transaction_product_binary.columns:\n",
    "            if prod1 != prod2:\n",
    "                # Transactions containing prod1\n",
    "                support_prod1 = product_support[prod1]\n",
    "                \n",
    "                # Transactions containing prod2\n",
    "                support_prod2 = product_support[prod2]\n",
    "                \n",
    "                # Transactions containing both\n",
    "                both = transaction_product_binary[transaction_product_binary[prod1] == 1][prod2].sum()\n",
    "                support_both = both / len(transaction_product_binary)\n",
    "                \n",
    "                # Calculate confidence\n",
    "                confidence = support_both / support_prod1 if support_prod1 > 0 else 0\n",
    "                \n",
    "                # Calculate lift\n",
    "                lift = confidence / support_prod2 if support_prod2 > 0 else 0\n",
    "                \n",
    "                associations.append({\n",
    "                    'product1': prod1,\n",
    "                    'product2': prod2,\n",
    "                    'support': support_both,\n",
    "                    'confidence': confidence,\n",
    "                    'lift': lift\n",
    "                })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    association_df = pd.DataFrame(associations)\n",
    "    \n",
    "    # Get product names\n",
    "    product_mapping = df[['ProductNo', 'ProductName']].drop_duplicates().set_index('ProductNo')['ProductName'].to_dict()\n",
    "    \n",
    "    # Add product names\n",
    "    association_df['product1_name'] = association_df['product1'].map(product_mapping)\n",
    "    association_df['product2_name'] = association_df['product2'].map(product_mapping)\n",
    "    \n",
    "    # Sort by lift\n",
    "    top_associations = association_df.sort_values('lift', ascending=False)\n",
    "    \n",
    "    return top_associations, product_support, product_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2905ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Collaborative Filtering Recommendation System\n",
    "def build_recommendation_system(df):\n",
    "    \"\"\"Build a product recommendation system based on collaborative filtering\"\"\"\n",
    "    # Create customer-product matrix with purchase quantities\n",
    "    customer_product_matrix = df.pivot_table(\n",
    "        index='CustomerNo', \n",
    "        columns='ProductNo', \n",
    "        values='Quantity', \n",
    "        aggfunc='sum',\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Calculate cosine similarity between products\n",
    "    product_similarity = cosine_similarity(customer_product_matrix.T)\n",
    "    \n",
    "    # Create a DataFrame for easier lookups\n",
    "    product_similarity_df = pd.DataFrame(\n",
    "        product_similarity, \n",
    "        index=customer_product_matrix.columns,\n",
    "        columns=customer_product_matrix.columns\n",
    "    )\n",
    "    \n",
    "    # Create product name mapping\n",
    "    product_names = df[['ProductNo', 'ProductName']].drop_duplicates().set_index('ProductNo')\n",
    "    \n",
    "    # Function to get recommendations for a product\n",
    "    def get_recommendations(product_id, n=5):\n",
    "        if product_id not in product_similarity_df.index:\n",
    "            return []\n",
    "        \n",
    "        # Get similarity scores\n",
    "        sim_scores = product_similarity_df[product_id]\n",
    "        \n",
    "        # Get top similar products (excluding itself)\n",
    "        similar_products = sim_scores.sort_values(ascending=False).index[1:n+1]\n",
    "        \n",
    "        # Get product names and similarity scores\n",
    "        recommendations = []\n",
    "        for prod_id in similar_products:\n",
    "            if prod_id in product_names.index:\n",
    "                prod_name = product_names.loc[prod_id, 'ProductName']\n",
    "                recommendations.append((prod_id, prod_name, sim_scores[prod_id]))\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    return get_recommendations, product_similarity_df, customer_product_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6292e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Purchase Behavior Prediction\n",
    "def build_purchase_prediction_model(df):\n",
    "    \"\"\"Build a model to predict high-value purchases\"\"\"\n",
    "    # Feature engineering\n",
    "    model_df = df.copy()\n",
    "    \n",
    "    # Encode cyclical features\n",
    "    model_df['DayOfWeek_sin'] = np.sin(2 * np.pi * model_df['DayOfWeek'] / 7)\n",
    "    model_df['DayOfWeek_cos'] = np.cos(2 * np.pi * model_df['DayOfWeek'] / 7)\n",
    "    model_df['Month_sin'] = np.sin(2 * np.pi * model_df['Month'] / 12)\n",
    "    model_df['Month_cos'] = np.cos(2 * np.pi * model_df['Month'] / 12)\n",
    "    \n",
    "    # Create target variable: high revenue purchases\n",
    "    median_revenue = model_df['Revenue'].median()\n",
    "    model_df['HighRevenue'] = (model_df['Revenue'] > median_revenue).astype(int)\n",
    "    \n",
    "    # Select features\n",
    "    features = [\n",
    "        'Price', 'Quantity', 'DayOfWeek_sin', 'DayOfWeek_cos', 'Month_sin', 'Month_cos',\n",
    "        'IsBlackFridayPeriod', 'IsChristmasPeriod', 'Total_Spent_Before', 'Topic'\n",
    "    ]\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = model_df[features].fillna(-1)\n",
    "    y = model_df['HighRevenue']\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train multiple models\n",
    "    models = {\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "        'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'XGBoost': xgb.XGBClassifier(random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Evaluate models\n",
    "    model_performance = {}\n",
    "    best_accuracy = 0\n",
    "    best_model_name = None\n",
    "    best_model = None\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "        \n",
    "        model_performance[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_name = name\n",
    "            best_model = model\n",
    "    \n",
    "    # Get feature importance for the best model\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': features,\n",
    "            'Importance': best_model.feature_importances_\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "    else:\n",
    "        feature_importance = None\n",
    "    \n",
    "    return best_model, best_model_name, model_performance, feature_importance, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6abc1acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Customer Lifetime Value Prediction\n",
    "def predict_customer_lifetime_value(df):\n",
    "    \"\"\"Build a regression model to predict customer lifetime value\"\"\"\n",
    "    # Aggregate data at customer level\n",
    "    customer_df = df.groupby('CustomerNo').agg({\n",
    "        'Revenue': 'sum',\n",
    "        'Quantity': 'sum',\n",
    "        'TransactionNo': 'nunique',\n",
    "        'ProductNo': 'nunique',\n",
    "        'Price': ['mean', 'max', 'min'],\n",
    "        'Days_Since_Last_Purchase': 'min',\n",
    "        'Avg_Purchase_Frequency': 'mean',\n",
    "        'Total_Spent_Before': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Flatten multi-level column names\n",
    "    customer_df.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in customer_df.columns]\n",
    "    \n",
    "    # Create additional features\n",
    "    customer_df['AvgTransactionValue'] = customer_df['Revenue_sum'] / customer_df['TransactionNo_nunique']\n",
    "    customer_df['ItemsPerTransaction'] = customer_df['Quantity_sum'] / customer_df['TransactionNo_nunique']\n",
    "    customer_df['PriceRange'] = customer_df['Price_max'] - customer_df['Price_min']\n",
    "    \n",
    "    # Handle missing values\n",
    "    customer_df = customer_df.fillna({\n",
    "        'Price_min': customer_df['Price_mean'],\n",
    "        'Price_max': customer_df['Price_mean'],\n",
    "        'Days_Since_Last_Purchase_min': 999,\n",
    "        'Avg_Purchase_Frequency_mean': 0,\n",
    "        'PriceRange': 0\n",
    "    })\n",
    "    \n",
    "    # Prepare data for modeling\n",
    "    X = customer_df.drop('Revenue_sum', axis=1)\n",
    "    y = customer_df['Revenue_sum']\n",
    "    \n",
    "    # Since our dataset is small, we'll use all data to train\n",
    "    # In a real scenario, we would split into train/test sets\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Predict CLV values\n",
    "    customer_df['PredictedCLV'] = model.predict(X)\n",
    "    \n",
    "    # Calculate feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return model, customer_df, feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a161d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Topic-Based Product Analysis\n",
    "def analyze_product_topics(df):\n",
    "    \"\"\"Analyze the performance of products by topic categories\"\"\"\n",
    "    if 'Topic' not in df.columns:\n",
    "        return None, None\n",
    "    \n",
    "    # Group by topic\n",
    "    topic_analysis = df.groupby('Topic').agg({\n",
    "        'Revenue': 'sum',\n",
    "        'Quantity': 'sum',\n",
    "        'TransactionNo': 'nunique',\n",
    "        'ProductNo': 'nunique',\n",
    "        'Price': 'mean'\n",
    "    }).sort_values('Revenue', ascending=False)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    topic_analysis['AvgRevenuePerTransaction'] = topic_analysis['Revenue'] / topic_analysis['TransactionNo']\n",
    "    topic_analysis['AvgQuantityPerTransaction'] = topic_analysis['Quantity'] / topic_analysis['TransactionNo']\n",
    "    topic_analysis['MarketShare'] = topic_analysis['Revenue'] / topic_analysis['Revenue'].sum() * 100\n",
    "    \n",
    "    # Find top products in each topic\n",
    "    top_products_by_topic = {}\n",
    "    for topic in df['Topic'].dropna().unique():\n",
    "        topic_products = df[df['Topic'] == topic].groupby('ProductNo').agg({\n",
    "            'Revenue': 'sum',\n",
    "            'Quantity': 'sum',\n",
    "            'ProductName': 'first'\n",
    "        }).sort_values('Revenue', ascending=False).head(5)\n",
    "        \n",
    "        top_products_by_topic[topic] = topic_products\n",
    "    \n",
    "    return topic_analysis, top_products_by_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2a9147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Price Sensitivity Analysis\n",
    "def analyze_price_sensitivity(df):\n",
    "    \"\"\"Analyze how price affects purchase quantity\"\"\"\n",
    "    # Group by product\n",
    "    product_analysis = df.groupby('ProductNo').agg({\n",
    "        'ProductName': 'first',\n",
    "        'Price': 'mean',\n",
    "        'Quantity': 'sum',\n",
    "        'Revenue': 'sum',\n",
    "        'TransactionNo': 'nunique'\n",
    "    })\n",
    "    \n",
    "    # Calculate price per unit\n",
    "    product_analysis['PricePerUnit'] = product_analysis['Price']\n",
    "    \n",
    "    # Calculate average quantity per transaction\n",
    "    product_analysis['AvgQuantityPerTransaction'] = product_analysis['Quantity'] / product_analysis['TransactionNo']\n",
    "    \n",
    "    # Calculate price rank (percentile)\n",
    "    product_analysis['PricePercentile'] = product_analysis['Price'].rank(pct=True)\n",
    "    \n",
    "    # Simulate price elasticity\n",
    "    # Note: Real elasticity calculation requires time series data with price changes\n",
    "    # This is a simplified simulation for illustration\n",
    "    product_analysis['SimulatedElasticity'] = -1 * (0.5 + product_analysis['PricePercentile'])\n",
    "    \n",
    "    # Calculate revenue impact with price changes\n",
    "    product_analysis['Revenue_10pct_PriceIncrease'] = product_analysis['Revenue'] * (1 + 0.1 * (1 + product_analysis['SimulatedElasticity']))\n",
    "    product_analysis['Revenue_10pct_PriceDecrease'] = product_analysis['Revenue'] * (1 - 0.1 * (1 + product_analysis['SimulatedElasticity']))\n",
    "    \n",
    "    # Price vs. Quantity correlation\n",
    "    price_quantity_corr = df[['Price', 'Quantity']].corr().iloc[0, 1]\n",
    "    \n",
    "    return product_analysis, price_quantity_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1be675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Customer Churn Risk Prediction\n",
    "def predict_churn_risk(df):\n",
    "    \"\"\"Predict which customers are at risk of churning\"\"\"\n",
    "    # Create customer-level features\n",
    "    customer_features = df.groupby('CustomerNo').agg({\n",
    "        'TransactionNo': 'nunique',\n",
    "        'Revenue': 'sum',\n",
    "        'Quantity': 'sum',\n",
    "        'Date': 'max',  # Latest purchase date\n",
    "        'Days_Since_Last_Purchase': 'min',\n",
    "        'Avg_Purchase_Frequency': 'mean',\n",
    "        'Total_Spent_Before': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Add recency feature\n",
    "    most_recent_date = df['Date'].max()\n",
    "    customer_features['DaysSinceLastPurchase'] = (most_recent_date - customer_features['Date']).dt.days\n",
    "    \n",
    "    # Define churn based on recency\n",
    "    # In a real scenario, this should be based on customer purchase patterns\n",
    "    customer_features['ChurnRisk'] = customer_features['DaysSinceLastPurchase'] > 30\n",
    "    \n",
    "    # Prepare features for the model\n",
    "    X = customer_features[[\n",
    "        'TransactionNo', 'Revenue', 'Quantity', 'DaysSinceLastPurchase',\n",
    "        'Days_Since_Last_Purchase', 'Avg_Purchase_Frequency', 'Total_Spent_Before'\n",
    "    ]].fillna(0)\n",
    "    \n",
    "    y = customer_features['ChurnRisk']\n",
    "    \n",
    "    # Build the model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Predict churn probability\n",
    "    customer_features['ChurnProbability'] = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Categorize churn risk\n",
    "    customer_features['ChurnRiskCategory'] = pd.cut(\n",
    "        customer_features['ChurnProbability'], \n",
    "        bins=[0, 0.3, 0.7, 1], \n",
    "        labels=['Low', 'Medium', 'High']\n",
    "    )\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    return customer_features, model, feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1aae9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Product Performance and Seasonality Analysis\n",
    "def analyze_product_performance(df):\n",
    "    \"\"\"Analyze product performance and identify seasonal patterns\"\"\"\n",
    "    # Group by product\n",
    "    product_performance = df.groupby('ProductNo').agg({\n",
    "        'ProductName': 'first',\n",
    "        'Revenue': 'sum',\n",
    "        'Quantity': 'sum',\n",
    "        'TransactionNo': 'nunique',\n",
    "        'Price': 'mean'\n",
    "    }).sort_values('Revenue', ascending=False)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    product_performance['AvgQuantityPerTransaction'] = product_performance['Quantity'] / product_performance['TransactionNo']\n",
    "    product_performance['AvgRevenuePerTransaction'] = product_performance['Revenue'] / product_performance['TransactionNo']\n",
    "    product_performance['MarketShare'] = product_performance['Revenue'] / product_performance['Revenue'].sum() * 100\n",
    "    \n",
    "    # Identify seasonal patterns\n",
    "    # Since our dataset has limited timespan, we'll create dummy seasonality data\n",
    "    # In a real scenario, we would analyze sales patterns across different time periods\n",
    "    \n",
    "    # Create a date-product pivot table (if we have enough date range)\n",
    "    if len(df['Date'].dt.date.unique()) > 1:\n",
    "        date_product_sales = df.pivot_table(\n",
    "            index='Date', \n",
    "            columns='ProductNo', \n",
    "            values='Quantity', \n",
    "            aggfunc='sum',\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        # Analyze day-of-week patterns\n",
    "        dow_sales = df.groupby(['DayOfWeek', 'DayName']).agg({\n",
    "            'Revenue': 'sum',\n",
    "            'Quantity': 'sum'\n",
    "        }).sort_values('Revenue', ascending=False)\n",
    "        \n",
    "        # Analyze month patterns\n",
    "        month_sales = df.groupby('Month').agg({\n",
    "            'Revenue': 'sum',\n",
    "            'Quantity': 'sum'\n",
    "        }).sort_values('Revenue', ascending=False)\n",
    "        \n",
    "        seasonality_analysis = {\n",
    "            'dow_sales': dow_sales,\n",
    "            'month_sales': month_sales\n",
    "        }\n",
    "    else:\n",
    "        seasonality_analysis = None\n",
    "    \n",
    "    return product_performance, seasonality_analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "966ec180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Customer Segmentation Based on RFM Analysis\n",
    "def rfm_segmentation(df):\n",
    "    \"\"\"Segment customers based on Recency, Frequency, and Monetary value\"\"\"\n",
    "    # Create customer-level features\n",
    "    rfm_df = df.groupby('CustomerNo').agg({\n",
    "        'Date': 'max',          # Recency: date of last purchase\n",
    "        'TransactionNo': 'nunique',  # Frequency: number of transactions\n",
    "        'Revenue': 'sum'        # Monetary: total spend\n",
    "    })\n",
    "    \n",
    "    # Calculate recency in days\n",
    "    most_recent_date = df['Date'].max()\n",
    "    rfm_df['Recency'] = (most_recent_date - rfm_df['Date']).dt.days\n",
    "    \n",
    "    # Rename columns\n",
    "    rfm_df = rfm_df.rename(columns={\n",
    "        'TransactionNo': 'Frequency',\n",
    "        'Revenue': 'Monetary'\n",
    "    })\n",
    "    \n",
    "    # Create RFM scores (1-5 scale, with 5 being the best)\n",
    "    # For Recency, lower is better, so we need to invert the quantiles\n",
    "    rfm_df['R_Score'] = pd.qcut(rfm_df['Recency'], 5, labels=False, duplicates='drop')\n",
    "    rfm_df['R_Score'] = 5 - rfm_df['R_Score']\n",
    "    \n",
    "    rfm_df['F_Score'] = pd.qcut(rfm_df['Frequency'].rank(method='first'), 5, labels=False, duplicates='drop') + 1\n",
    "    rfm_df['M_Score'] = pd.qcut(rfm_df['Monetary'].rank(method='first'), 5, labels=False, duplicates='drop') + 1\n",
    "    \n",
    "    # Calculate RFM score\n",
    "    rfm_df['RFM_Score'] = rfm_df['R_Score'] * 100 + rfm_df['F_Score'] * 10 + rfm_df['M_Score']\n",
    "    \n",
    "    # Segment customers based on RFM score\n",
    "    rfm_df['RFM_Segment'] = pd.cut(\n",
    "        rfm_df['RFM_Score'],\n",
    "        bins=[0, 150, 250, 350, 450, 550],\n",
    "        labels=['Lost Customers', 'At Risk', 'Average Customers', 'Loyal Customers', 'Champions']\n",
    "    )\n",
    "    \n",
    "    return rfm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8de165f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 4 customer segments\n",
      "Cluster analysis:\n",
      "         TransactionNo_nunique    Revenue_sum  Quantity_sum  \\\n",
      "Cluster                                                       \n",
      "3                   131.200000  314071.184000  27738.200000   \n",
      "2                     7.673392   22172.706166   1933.514502   \n",
      "1                     6.000000   16419.112857   1301.904762   \n",
      "0                     3.087243    4346.947567    374.321033   \n",
      "\n",
      "         ProductNo_nunique  Price_mean  AvgBasketSize  AvgTransactionValue  \n",
      "Cluster                                                                     \n",
      "3              1225.000000   11.943418     270.310479          3066.154674  \n",
      "2               234.194199   12.719684     412.477053          4852.560854  \n",
      "1               123.761905   69.088692     192.432868          2678.995105  \n",
      "0                48.977069   13.049606     125.706889          1467.377260  \n",
      "Explained variance: [0.44605916 0.2308269  0.1672437 ]\n",
      "\n",
      "Customer segment sample:\n",
      "            TransactionNo_nunique  Revenue_sum  Quantity_sum  \\\n",
      "CustomerNo                                                     \n",
      "12004.0                         1      1509.60           104   \n",
      "12006.0                         1        24.76             2   \n",
      "12008.0                         1      4606.61           333   \n",
      "\n",
      "            ProductNo_nunique  Price_mean  Price_max  AvgBasketSize  \\\n",
      "CustomerNo                                                            \n",
      "12004.0                    56   15.042679      31.56          104.0   \n",
      "12006.0                     1   12.380000      12.38            2.0   \n",
      "12008.0                   201   14.039353      35.83          333.0   \n",
      "\n",
      "            AvgTransactionValue  Cluster          SegmentName  \n",
      "CustomerNo                                                     \n",
      "12004.0                 1509.60        0  Low-Value Customers  \n",
      "12006.0                   24.76        0  Low-Value Customers  \n",
      "12008.0                 4606.61        2    Regular Customers  \n"
     ]
    }
   ],
   "source": [
    "customer_segments, cluster_analysis, cluster_names, explained_variance = customer_segmentation(df)\n",
    "print(f\"Created {len(cluster_analysis)} customer segments\")\n",
    "print(\"Cluster analysis:\")\n",
    "print(cluster_analysis)\n",
    "print(f\"Explained variance: {explained_variance}\")\n",
    "print(\"\\nCustomer segment sample:\")\n",
    "print(customer_segments.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a43b719",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_associations, product_support, product_mapping = product_association_analysis(df)\n",
    "print(\"Top 5 product associations by lift:\")\n",
    "print(top_associations[['product1_name', 'product2_name', 'support', 'confidence', 'lift']].head(5))\n",
    "print(\"\\nTop 5 products by support (popularity):\")\n",
    "top_5_products = product_support.sort_values(ascending=False).head(5)\n",
    "for prod_id, support in top_5_products.items():\n",
    "    prod_name = product_mapping.get(prod_id, f\"Product {prod_id}\")\n",
    "    print(f\"{prod_name}: {support:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a05d3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for Set Of 2 Wooden Market Crates:\n",
      "- Georgian Trinket Box (similarity: 0.51)\n",
      "- Pink Pot Plant Candle (similarity: 0.48)\n",
      "- Tall Rococo Candle Holder (similarity: 0.48)\n",
      "- Bathroom Scales Footprints In Sand (similarity: 0.47)\n",
      "- Empire Union Jack Tv Dinner Tray (similarity: 0.47)\n",
      "\n",
      "Customer-product matrix shape: (4634, 3746)\n"
     ]
    }
   ],
   "source": [
    "get_recommendations, product_similarity_df, customer_product_matrix = build_recommendation_system(df)\n",
    "# Get a sample product to recommend for\n",
    "sample_product = df['ProductNo'].iloc[0]\n",
    "sample_product_name = df[df['ProductNo'] == sample_product]['ProductName'].iloc[0]\n",
    "print(f\"Recommendations for {sample_product_name}:\")\n",
    "recommendations = get_recommendations(sample_product)\n",
    "for prod_id, prod_name, similarity in recommendations:\n",
    "    print(f\"- {prod_name} (similarity: {similarity:.2f})\")\n",
    "print(\"\\nCustomer-product matrix shape:\", customer_product_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a93dbb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: RandomForest\n",
      "\n",
      "Model performance:\n",
      "RandomForest: Accuracy=1.00, F1=1.00\n",
      "GradientBoosting: Accuracy=1.00, F1=1.00\n",
      "LogisticRegression: Accuracy=0.97, F1=0.97\n",
      "XGBoost: Accuracy=1.00, F1=1.00\n",
      "\n",
      "Feature importance:\n",
      "               Feature  Importance\n",
      "1             Quantity    0.881918\n",
      "0                Price    0.092050\n",
      "8   Total_Spent_Before    0.010058\n",
      "5            Month_cos    0.005207\n",
      "6  IsBlackFridayPeriod    0.004407\n",
      "4            Month_sin    0.002879\n",
      "9                Topic    0.001329\n",
      "2        DayOfWeek_sin    0.001276\n",
      "3        DayOfWeek_cos    0.000788\n",
      "7    IsChristmasPeriod    0.000088\n"
     ]
    }
   ],
   "source": [
    "purchase_model, model_name, model_performance, feature_importance, features = build_purchase_prediction_model(df)\n",
    "print(f\"Best model: {model_name}\")\n",
    "print(\"\\nModel performance:\")\n",
    "for model, metrics in model_performance.items():\n",
    "    print(f\"{model}: Accuracy={metrics['accuracy']:.2f}, F1={metrics['f1']:.2f}\")\n",
    "print(\"\\nFeature importance:\")\n",
    "if feature_importance is not None:\n",
    "    print(feature_importance.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72d8a1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLV prediction feature importance:\n",
      "                        Feature  Importance\n",
      "0                  Quantity_sum    0.923989\n",
      "8       Total_Spent_Before_mean    0.017414\n",
      "11                   PriceRange    0.013192\n",
      "2             ProductNo_nunique    0.012599\n",
      "3                    Price_mean    0.010875\n",
      "4                     Price_max    0.007544\n",
      "1         TransactionNo_nunique    0.006872\n",
      "5                     Price_min    0.003534\n",
      "10          ItemsPerTransaction    0.001943\n",
      "7   Avg_Purchase_Frequency_mean    0.000944\n",
      "\n",
      "Customer CLV sample:\n",
      "            Revenue_sum  PredictedCLV\n",
      "CustomerNo                           \n",
      "12004.0         1509.60     1502.2996\n",
      "12006.0           24.76       24.7802\n",
      "12008.0         4606.61     4540.7769\n",
      "12013.0           69.96       70.1822\n",
      "12024.0          149.52      149.3336\n"
     ]
    }
   ],
   "source": [
    "# 6. Customer Lifetime Value Prediction\n",
    "# This should be in a cell by itself\n",
    "clv_model, customer_clv, clv_feature_importance = predict_customer_lifetime_value(df)\n",
    "print(\"CLV prediction feature importance:\")\n",
    "print(clv_feature_importance.head(10))\n",
    "print(\"\\nCustomer CLV sample:\")\n",
    "print(customer_clv[['Revenue_sum', 'PredictedCLV']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e77a2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic analysis:\n",
      "          Revenue  Quantity  MarketShare\n",
      "Topic                                   \n",
      "0      9997736.99    850406    27.515268\n",
      "2      7400662.93    664140    20.367732\n",
      "1      7261038.17    627135    19.983464\n",
      "4      5925700.81    537874    16.308415\n",
      "3      5750094.82    467267    15.825121\n",
      "\n",
      "Top products by topic:\n",
      "\n",
      "Topic 1:\n",
      "- Pack Of 72 Retrospot Cake Cases: Revenue=$180931.96, Quantity=16855\n",
      "- Party Bunting: Revenue=$139201.99, Quantity=8775\n",
      "- Pack Of 60 Pink Paisley Cake Cases: Revenue=$97366.40, Quantity=9148\n",
      "- 60 Teatime Fairy Cake Cases: Revenue=$95029.59, Quantity=8925\n",
      "- Spotty Bunting: Revenue=$85871.22, Quantity=5554\n",
      "\n",
      "Topic 4:\n",
      "- Mini Paint Set Vintage: Revenue=$97266.87, Quantity=9134\n",
      "- Jumbo Shopper Vintage Red Paisley: Revenue=$95069.03, Quantity=7683\n",
      "- 6 Ribbons Rustic Charm: Revenue=$87428.85, Quantity=7384\n",
      "- Paper Chain Kit 50'S Christmas: Revenue=$87006.41, Quantity=7084\n",
      "- Vintage Snap Cards: Revenue=$80606.83, Quantity=7635\n",
      "\n",
      "Topic 3:\n",
      "- Jam Making Set Printed: Revenue=$115749.71, Quantity=10042\n",
      "- Jam Making Set With Jars: Revenue=$89142.13, Quantity=6270\n",
      "- Set Of 4 Pantry Jelly Moulds: Revenue=$88708.14, Quantity=7897\n",
      "- Red Toadstool Led Night Light: Revenue=$77443.20, Quantity=6699\n",
      "- Set Of 60 Pantry Design Cake Cases: Revenue=$77360.08, Quantity=7321\n",
      "\n",
      "Topic 0:\n",
      "- Cream Hanging Heart T-Light Holder: Revenue=$253202.20, Quantity=19688\n",
      "- Assorted Colour Bird Ornament: Revenue=$205222.32, Quantity=17594\n",
      "- Popcorn Holder: Revenue=$144024.29, Quantity=13279\n",
      "- Victorian Glass Hanging T-Light: Revenue=$118434.94, Quantity=10292\n",
      "- Antique Silver T-Light Glass: Revenue=$108001.74, Quantity=9701\n",
      "\n",
      "Topic 2:\n",
      "- Regency Cakestand 3 Tier: Revenue=$229616.36, Quantity=10208\n",
      "- Jumbo Bag Red Retrospot: Revenue=$98437.11, Quantity=15638\n",
      "- Jumbo Storage Bag Suki: Revenue=$94024.02, Quantity=7512\n",
      "- Red Retrospot Charlotte Bag: Revenue=$87637.34, Quantity=7880\n",
      "- Wooden Picture Frame White Finish: Revenue=$84165.88, Quantity=6669\n"
     ]
    }
   ],
   "source": [
    "topic_analysis, top_products_by_topic = analyze_product_topics(df)\n",
    "if topic_analysis is not None:\n",
    "    print(\"Topic analysis:\")\n",
    "    print(topic_analysis[['Revenue', 'Quantity', 'MarketShare']])\n",
    "    print(\"\\nTop products by topic:\")\n",
    "    for topic, products in top_products_by_topic.items():\n",
    "        print(f\"\\nTopic {topic}:\")\n",
    "        for idx, row in products.iterrows():\n",
    "            print(f\"- {row['ProductName']}: Revenue=${row['Revenue']:.2f}, Quantity={row['Quantity']}\")\n",
    "else:\n",
    "    print(\"Topic analysis not available (Topic column may be missing)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec5e7015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price-Quantity correlation: -0.13\n",
      "\n",
      "Price analysis sample (most elastic products):\n",
      "                                 ProductName       Price  Quantity  \\\n",
      "ProductNo                                                            \n",
      "22656           Vintage Blue Kitchen Cabinet  637.478000        26   \n",
      "22655            Vintage Red Kitchen Cabinet  624.080000        60   \n",
      "22827      Rustic Seventeen Drawer Sideboard  172.277308        35   \n",
      "22828           Regency Mirror With Shutters  170.588571        10   \n",
      "22823          Chest Natural Wood 20 Drawers  131.281538        24   \n",
      "\n",
      "           SimulatedElasticity  \n",
      "ProductNo                       \n",
      "22656                -1.500000  \n",
      "22655                -1.499733  \n",
      "22827                -1.499466  \n",
      "22828                -1.499199  \n",
      "22823                -1.498932  \n"
     ]
    }
   ],
   "source": [
    "#higher price ->lower demand\n",
    "# This should be in a cell by itself\n",
    "price_analysis, price_quantity_corr = analyze_price_sensitivity(df)\n",
    "print(f\"Price-Quantity correlation: {price_quantity_corr:.2f}\")\n",
    "print(\"\\nPrice analysis sample (most elastic products):\")\n",
    "elastic_products = price_analysis.sort_values('SimulatedElasticity').head(5)\n",
    "print(elastic_products[['ProductName', 'Price', 'Quantity', 'SimulatedElasticity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4eb0ddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Churn risk by category:\n",
      "ChurnRiskCategory\n",
      "High      2922\n",
      "Low        508\n",
      "Medium       0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Churn feature importance:\n",
      "                    Feature  Importance\n",
      "3     DaysSinceLastPurchase    0.810737\n",
      "0             TransactionNo    0.071939\n",
      "1                   Revenue    0.036494\n",
      "2                  Quantity    0.034756\n",
      "4  Days_Since_Last_Purchase    0.023596\n",
      "\n",
      "High-risk customer sample:\n",
      "            ChurnProbability  Revenue  TransactionNo\n",
      "CustomerNo                                          \n",
      "12004.0                  1.0  1509.60              1\n",
      "12006.0                  1.0    24.76              1\n",
      "12008.0                  1.0  4606.61              1\n",
      "12013.0                  1.0    69.96              1\n",
      "12024.0                  1.0   149.52              1\n"
     ]
    }
   ],
   "source": [
    "churn_data, churn_model, churn_feature_importance = predict_churn_risk(df)\n",
    "print(\"Churn risk by category:\")\n",
    "print(churn_data['ChurnRiskCategory'].value_counts())\n",
    "print(\"\\nChurn feature importance:\")\n",
    "print(churn_feature_importance.head(5))\n",
    "print(\"\\nHigh-risk customer sample:\")\n",
    "high_risk = churn_data[churn_data['ChurnRiskCategory'] == 'High'].head(5)\n",
    "print(high_risk[['ChurnProbability', 'Revenue', 'TransactionNo']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff7e6782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 products by revenue:\n",
      "                                  ProductName    Revenue  Quantity  \\\n",
      "ProductNo                                                            \n",
      "85123A     Cream Hanging Heart T-Light Holder  253202.20     19688   \n",
      "22423                Regency Cakestand 3 Tier  229616.36     10208   \n",
      "84879           Assorted Colour Bird Ornament  205222.32     17594   \n",
      "21212         Pack Of 72 Retrospot Cake Cases  180931.96     16855   \n",
      "22197                          Popcorn Holder  144024.29     13279   \n",
      "\n",
      "           MarketShare  \n",
      "ProductNo               \n",
      "85123A        0.696850  \n",
      "22423         0.631939  \n",
      "84879         0.564803  \n",
      "21212         0.497952  \n",
      "22197         0.396376  \n",
      "\n",
      "Day of week sales pattern:\n",
      "                      Revenue  Quantity\n",
      "DayOfWeek DayName                      \n",
      "6         Sun      7563117.02    659554\n",
      "4         Fri      6711706.82    575292\n",
      "5         Sat      6447200.50    565109\n",
      "3         Thu      6072805.83    525126\n",
      "0         Mon      5702889.26    490201\n",
      "2         Wed      3837514.29    331540\n",
      "\n",
      "Month sales pattern:\n",
      "          Revenue  Quantity\n",
      "Month                      \n",
      "11     4855724.96    464709\n",
      "10     4322870.76    360725\n",
      "9      3944362.88    333802\n",
      "12     3615895.82    352347\n",
      "7      2711542.03    232013\n",
      "5      2701137.47    225882\n",
      "8      2676151.60    229174\n",
      "3      2572774.54    210059\n",
      "6      2559192.43    214720\n",
      "1      2319931.04    189286\n",
      "4      2129997.56    177094\n",
      "2      1925652.63    157011\n"
     ]
    }
   ],
   "source": [
    "product_performance, seasonality_analysis = analyze_product_performance(df)\n",
    "print(\"Top 5 products by revenue:\")\n",
    "print(product_performance.head(5)[['ProductName', 'Revenue', 'Quantity', 'MarketShare']])\n",
    "if seasonality_analysis is not None:\n",
    "    print(\"\\nDay of week sales pattern:\")\n",
    "    print(seasonality_analysis['dow_sales'])\n",
    "    print(\"\\nMonth sales pattern:\")\n",
    "    print(seasonality_analysis['month_sales'])\n",
    "else:\n",
    "    print(\"\\nSeasonality analysis not available (limited date range)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f96f9f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFM Segment distribution:\n",
      "RFM_Segment\n",
      "Lost Customers       909\n",
      "At Risk              881\n",
      "Average Customers    842\n",
      "Loyal Customers      787\n",
      "Champions            756\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Customers by RFM score (sample):\n",
      "            Recency  Frequency  Monetary  RFM_Score     RFM_Segment\n",
      "CustomerNo                                                         \n",
      "12004.0         227          1   1509.60        112  Lost Customers\n",
      "12006.0         218          1     24.76        111  Lost Customers\n",
      "12008.0         276          1   4606.61        113  Lost Customers\n",
      "12013.0         359          1     69.96        111  Lost Customers\n",
      "12024.0         176          1    149.52        211         At Risk\n"
     ]
    }
   ],
   "source": [
    "# 11. RFM Segmentation\n",
    "# This should be in a cell by itself\n",
    "rfm_segments = rfm_segmentation(df)\n",
    "print(\"RFM Segment distribution:\")\n",
    "print(rfm_segments['RFM_Segment'].value_counts())\n",
    "print(\"\\nCustomers by RFM score (sample):\")\n",
    "print(rfm_segments[['Recency', 'Frequency', 'Monetary', 'RFM_Score', 'RFM_Segment']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9984ea4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de53cc26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
